{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "VgvvGLtO-zqg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 615
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1957,
     "status": "ok",
     "timestamp": 1530134351067,
     "user": {
      "displayName": "ANDRES CARVALLO D.",
      "photoUrl": "//lh3.googleusercontent.com/-H8zsBHE3TLo/AAAAAAAAAAI/AAAAAAAAAEA/tfwxTUT2fzY/s50-c-k-no/photo.jpg",
      "userId": "110873878051007895451"
     },
     "user_tz": 240
    },
    "id": "S_GBebZJcWVI",
    "outputId": "5cb8ac6f-4e3f-4b53-b5bf-1b533702265b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 20, 20, 32)        2080      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 20, 20, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 10, 10, 64)        32832     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 10, 10, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               3277312   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 1539      \n",
      "=================================================================\n",
      "Total params: 3,350,691\n",
      "Trainable params: 3,350,691\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), input_shape=(80, 80, 1..., strides=(4, 4), padding=\"same\")`\n",
      "  if __name__ == '__main__':\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (4, 4), strides=(2, 2), padding=\"same\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), strides=(1, 1), padding=\"same\")`\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Convolution2D, Flatten, MaxPooling2D, Reshape\n",
    "from keras import optimizers\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(32, 8, 8, subsample=(4, 4), border_mode='same',input_shape=(80,80,1)))  #80*80*4\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, 4, 4, subsample=(2, 2), border_mode='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, 3, 3, subsample=(1, 1), border_mode='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(3))\n",
    "\n",
    "adam = optimizers.Adam(lr=LEARNING_RATE)\n",
    "model.compile(loss='mse',optimizer=adam)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ZO0XYkv1WnWj"
   },
   "outputs": [],
   "source": [
    "def load_model():\n",
    "  model.load_weights(\"model.h5\")\n",
    "  adam = optimizers.Adam(lr=LEARNING_RATE)\n",
    "  model.compile(loss='mse',optimizer=adam)\n",
    "\n",
    "load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sz2MDii9VBJI"
   },
   "source": [
    "### Con entrenamiento dentro de deque()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "GvZ7jYarVODI"
   },
   "outputs": [],
   "source": [
    "mode = 'Run'\n",
    "#mode = 'observe'\n",
    "\n",
    "import random \n",
    "import gym \n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import json \n",
    "import random\n",
    "from skimage import color\n",
    "from skimage import transform \n",
    "from skimage import exposure\n",
    "import time \n",
    "\n",
    "def preProcessImage(img):\n",
    "    Img = color.rgb2gray(img)\n",
    "    Img = transform.resize(Img,(80,80))\n",
    "    return Img \n",
    "  \n",
    "\n",
    "# Pong es el juego \n",
    "env = gym.make('Pong-v0')\n",
    "ob = env.reset()  \n",
    "\n",
    "# accion inicial random  \n",
    "#a_t = random.choice([0,2,3])\n",
    "\n",
    "# accion inicial en Pong \n",
    "\n",
    "ACTIONS = 3 # number of valid actions\n",
    "\n",
    "P_ACTIONS = [0,2,3]\n",
    "\n",
    "GAMMA = 0.95 # decay rate of past observations\n",
    "OBSERVATION = 300 # timesteps to observe before training\n",
    "EXPLORE = 100000 # frames over which to anneal epsilon\n",
    "\n",
    "FINAL_EPSILON = 0.1 # final value of epsilon\n",
    "INITIAL_EPSILON = 0.5 # starting value of epsilon\n",
    "\n",
    "REPLAY_MEMORY = 300 # number of previous transitions to remember\n",
    "BATCH = 32 # size of minibatch\n",
    "FRAME_PER_ACTION = 1\n",
    "\n",
    "img_rows , img_cols = 80, 80\n",
    "#Convert image into Black and white\n",
    "img_channels = 4 #We stack 4 frames\n",
    "\n",
    "\n",
    "\n",
    "def trainNetwork(model,mode):\n",
    "    # open up a game state to communicate with emulator\n",
    "    env = gym.make('Pong-v0')\n",
    "    \n",
    "    ob = env.reset() \n",
    "\n",
    "    # store the previous observations in replay memory\n",
    "    D = deque()\n",
    "    \n",
    "    env.render()\n",
    "    \n",
    "    # get the first state by doing nothing and preprocess the image to 80x80x4\n",
    "    #do_nothing = np.zeros(ACTIONS)\n",
    "    do_nothing = 0\n",
    "    #x_t, r_0, terminal = game_state.frame_step(do_nothing)\n",
    "    x_t, r_0, terminal, _ = env.step(do_nothing) # porque la accion 0 es 0 y es hacer nada\n",
    "    x_t = preProcessImage(x_t) # modifica tama√±o y grey de la imagen\n",
    "    \n",
    "    # normaliza imagen \n",
    "    x_t = exposure.rescale_intensity(x_t, out_range=(0, 255))\n",
    "\n",
    "    x_t = x_t / 255.0\n",
    "\n",
    "    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)\n",
    "    #print (s_t.shape)\n",
    "    \n",
    "    #In Keras, need to reshape\n",
    "    s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2])  #1*80*80*4\n",
    "\n",
    "    if mode == 'Run':\n",
    "        OBSERVE = 999999999    #We keep observe, never train\n",
    "        epsilon = FINAL_EPSILON\n",
    "        print (\"Now we load weights\")\n",
    "        #model.load_weights(\"model.h5\")\n",
    "        #adam = optimizers.Adam(lr=LEARNING_RATE)\n",
    "        #model.compile(loss='mse',optimizer=adam)\n",
    "        print (\"Weights loaded successfully. En realidad no lo cargamos  y usamos el mismo\") \n",
    "        \n",
    "    else:                       #We go to training mode\n",
    "        OBSERVE = OBSERVATION\n",
    "        epsilon = INITIAL_EPSILON\n",
    "    \n",
    "    reward_acumulado = 0 \n",
    "    t = 0\n",
    "    \n",
    "    opcion = ''\n",
    "    \n",
    "    while (t <= OBSERVATION + EXPLORE):\n",
    "        loss = 0\n",
    "        Q_sa = 0\n",
    "        r_t = 0\n",
    "        #a_t = np.zeros([ACTIONS])\n",
    "        a_t = 0 \n",
    "        \n",
    "        \n",
    "        #choose an action epsilon greedy\n",
    "        if t % FRAME_PER_ACTION == 0:\n",
    "          if random.random() <= epsilon:\n",
    "            opcion = 'random'\n",
    "            #print(\"----------Random Action----------\")\n",
    "            a_t = random.choice([0,1,2])\n",
    "            \n",
    "          else:\n",
    "            #print(\"++++++++++Predicted Action----------\")\n",
    "            q = model.predict(s_t)     #input a stack of 4 images, get the prediction\n",
    "            max_Q = np.argmax(q)\n",
    "            opcion = 'prediccion'\n",
    "            a_t = max_Q\n",
    "            \n",
    "\n",
    "        #We reduced the epsilon gradually\n",
    "        if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
    "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE\n",
    "\n",
    "        #run the selected action and observed next state and reward\n",
    "        x_t1_colored, r_t, terminal,_ = env.step(P_ACTIONS[a_t]) # decia env.frame\n",
    "        \n",
    "        # escala de grises y resize \n",
    "        x_t1 = preProcessImage(x_t1_colored)\n",
    "        \n",
    "        # normaliza imagen \n",
    "        x_t1 = exposure.rescale_intensity(x_t1, out_range=(0, 255))\n",
    "        x_t1 = x_t1 / 255.0\n",
    "        \n",
    "                      \n",
    "        # reshape keras \n",
    "        x_t1 = x_t1.reshape(1, x_t1.shape[0], x_t1.shape[1], 1) #1x80x80x1\n",
    "        s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3)\n",
    "\n",
    "        # store the transition in D\n",
    "        D.append((s_t, a_t, r_t, s_t1, terminal))\n",
    "        \n",
    "        #revisar esto dl reward\n",
    "        reward_acumulado += r_t \n",
    "                                            \n",
    "        if len(D) > REPLAY_MEMORY:\n",
    "            D.popleft()\n",
    "\n",
    "        #only train if done observing\n",
    "        if t > OBSERVE:\n",
    "            \n",
    "            minibatch = random.sample(D,BATCH)\n",
    "            inputs = np.zeros((BATCH,s_t.shape[1],s_t.shape[2],s_t.shape[3]))\n",
    "            targets = np.zeros((inputs.shape[0],3))\n",
    "       \n",
    "            state_t, action_t, reward_t, state_t1, terminal_t1 = zip(*minibatch)\n",
    "            \n",
    "            #print('TIME STEP: ', t, 'largo de D: ', len(D))\n",
    "\n",
    "            state_t = np.concatenate(state_t)\n",
    "            state_t1 = np.concatenate(state_t1)\n",
    "                      \n",
    "            targets = model.predict(state_t)\n",
    "            \n",
    "            print(\"--------- Targets step: \",t)\n",
    "            print(targets)\n",
    "            print(\"-------------------------\")\n",
    "              \n",
    "            Q_sa = model.predict(state_t1)\n",
    "            \n",
    "            if terminal_t1:\n",
    "              targets[range(BATCH), action_t] = reward_t\n",
    "              print('suma reward', reward_t)\n",
    "              \n",
    "            else:\n",
    "              targets[range(BATCH), action_t] = reward_t + GAMMA*np.max(Q_sa)\n",
    "              \n",
    "            \n",
    "            #targets = [P_ACTIONS[np.argmax(x)] for x in targets]\n",
    "              \n",
    "                        \n",
    "            loss += model.train_on_batch(state_t, targets)\n",
    "\n",
    "            #print('entrenando*****, loss: ', loss, '************************')\n",
    "\n",
    "        s_t = s_t1\n",
    "        t = t + 1\n",
    "\n",
    "        # save progress every 10000 iterations\n",
    "        if t % 50000 == 0:\n",
    "            print(\"Now we save model\")\n",
    "            model.save(\"model.h5\", overwrite=True)\n",
    "                \n",
    "            # guardamos modelo en el drive \n",
    "            save_to_drive('model.h5')\n",
    "\n",
    "        # print info\n",
    "        state = \"\"\n",
    "        if t <= OBSERVE:\n",
    "            state = \"observe\"\n",
    "        elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n",
    "            state = \"explore\"\n",
    "        else:\n",
    "            state = \"train\"\n",
    "\n",
    "        #print(\"TIMESTEP\", t, \"/ STATE\", state, \\\n",
    "         #   \"/ EPSILON\", epsilon, \"/ ACTION\", a_t, \"/ REWARD\", r_t, \"/ R_ACUM\", reward_acumulado, \\\n",
    "          #  \"/ Q_MAX \" , np.max(Q_sa), \"/ Loss \", loss)\n",
    "        \n",
    "        if terminal==True :\n",
    "          print(t,' - reward acumulado', reward_acumulado, 'STATE: ', state, 'EPSILON', epsilon, 'LAST ACTION', a_t, 'OPCION', opcion)\n",
    "          reward_acumulado = 0\n",
    "          terminal = False\n",
    "          ob = env.reset()\n",
    "          \n",
    "    print(\"Episode finished!\")\n",
    "    print(\"************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vAcOOBkdruag"
   },
   "source": [
    "## NUEVO .ENFOQUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "GC0D0TahrtpA"
   },
   "outputs": [],
   "source": [
    "import random \n",
    "import gym \n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import json \n",
    "import random\n",
    "from skimage import color\n",
    "from skimage import transform \n",
    "from skimage import exposure\n",
    "import time \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def preProcessImage(img):\n",
    "    Img = color.rgb2gray(img)\n",
    "    Img = transform.resize(Img,(80,80))\n",
    "    return Img \n",
    "\n",
    "def aplicar_maxpool(matrix):\n",
    "  lala = matrix.reshape(1,80,80,4) #le hace reshape al vector concadenado\n",
    "  lala = np.amax(lala, axis=3) # calcula el maximo\n",
    "  lala = lala.reshape(1,80,80,1) #le hace reshape al vector concadenado\n",
    "  #lala = lala.reshape(80,80) #lo deja como imagen de 80x80 (no se usa porque keras recibe 1x80x80x1)\n",
    "  #plt.imshow(lala, interpolation='nearest')\n",
    "  #plt.show()\n",
    "  return lala\n",
    "\n",
    "def mostrarImagen(imagen) : #la entrada debe tener un shape de 1x80x80x1\n",
    "  lala = imagen.reshape(80,80) #lo deja como imagen de 80x80 (no se usa porque keras recibe 1x80x80x1)\n",
    "  plt.imshow(lala, interpolation='nearest')\n",
    "  plt.show()\n",
    "\n",
    "def normalizar(numero):\n",
    "  result = 0 \n",
    "  if numero < 0:\n",
    "    result = -0.8 \n",
    "  else:\n",
    "    result = 1 \n",
    "  \n",
    "  return result \n",
    "    \n",
    "  \n",
    "\n",
    "# Pong es el juego \n",
    "env = gym.make('Pong-v0')\n",
    "ob = env.reset()  \n",
    "\n",
    "ACTIONS = 3 # numero de acciones validas\n",
    "\n",
    "P_ACTIONS = [0,2,3]\n",
    "\n",
    "GAMMA = 0.5 # decay rate of past observations\n",
    "OBSERVATION = 300 # timesteps to observe before training\n",
    "EXPLORE = 250000 # frames over which to anneal epsilon\n",
    "\n",
    "FINAL_EPSILON = 0.001 # final value of epsilon\n",
    "INITIAL_EPSILON = .1 # starting value of epsilon\n",
    "\n",
    "REPLAY_MEMORY = 300 # number of previous transitions to remember\n",
    "BATCH = 10 # size of minibatch\n",
    "FRAME_PER_ACTION = 4\n",
    "\n",
    "img_rows , img_cols = 80, 80\n",
    "#Convert image into Black and white\n",
    "img_channels = 4 #We stack 4 frames\n",
    "\n",
    "\n",
    "D = deque()\n",
    "\n",
    "\n",
    "def trainNetwork2(model,epsilon,mode):\n",
    "    reward_acum = 0\n",
    "    loss = 0\n",
    "    entrenando = ''\n",
    "    env = gym.make('Pong-v0')\n",
    "    ob = env.reset() \n",
    "  \n",
    "    # store the previous observations in replay memory\n",
    "    D.clear()\n",
    "    D_por_episodio = list()\n",
    "        \n",
    "    last_4_frames = []\n",
    "\n",
    "    for i in range(4):\n",
    "      x_t, r_0, terminal, _ = env.step(0) # porque la accion 0 es 0 y es hacer nada\n",
    "      x_t = preProcessImage(x_t) # modifica tama√±o y grey de la imagen\n",
    "      last_4_frames.append(x_t)\n",
    "   \n",
    "    s_t = np.stack((last_4_frames[0],last_4_frames[1], last_4_frames[2], last_4_frames[3]), axis=2)\n",
    "    s_t = aplicar_maxpool(s_t) # S_T0 CONTIENE EL ESTADO INICIAL SUB 0 CON SHAPE 1x80x80x1\n",
    "    \n",
    "    lista_estados = []\n",
    "    r_t1 = r_0\n",
    "    for t in range (EXPLORE) :\n",
    "        \n",
    "        env.render()\n",
    "        \n",
    "        time.sleep(0.1)\n",
    "        \n",
    "        #SELECCIONAR UNA ACCION ALEATORIA CON PROBABILIDAD EPSILON\n",
    "        if random.random() <= epsilon:\n",
    "          opcion = 'random'\n",
    "          a_t = random.choice([0,1,2])\n",
    "            \n",
    "        else:\n",
    "          opcion = 'prediccion'\n",
    "          q = model.predict(s_t)    \n",
    "          max_Q = np.argmax(q)\n",
    "          \n",
    "          a_t = max_Q\n",
    "        #JUEGA 4 VECES CON LA MISMA ACCION\n",
    "        r_t1 = 0\n",
    "        r_instanstanteo_4 = 0\n",
    "        for it in range(4):\n",
    "          x_t1, r_t1, terminal1, _ = env.step(P_ACTIONS[a_t])\n",
    "          x_t1 = preProcessImage(x_t1)\n",
    "          \n",
    "          lista_estados.append(x_t1)\n",
    "          \n",
    "          if r_t1 != 0:\n",
    "            r_instanstanteo_4 = r_t1\n",
    "          #r_t1 = max(0,r_t1)\n",
    "          \n",
    "          if r_t1 >= 0:\n",
    "              reward_acum += r_t1\n",
    "        \n",
    "        r_t1 = r_instanstanteo_4   \n",
    "          # si entre los 4 juegos uno de ellos es terminal, se sale del juego y se queda con lo que encontro hasta ahi\n",
    "        \n",
    "                    \n",
    "        s_t1 = np.stack(lista_estados, axis=2)\n",
    "        s_t1 = aplicar_maxpool(s_t1)\n",
    "        lista_estados=[]\n",
    "        \n",
    "        #mostrarImagen(s_t1)\n",
    "        \n",
    "        D_por_episodio.append((s_t, a_t, r_t1, s_t1, terminal1))\n",
    "        s_t = s_t1\n",
    "        r_t = r_t1\n",
    "        \n",
    "        # REVISAR ESTO PORQUE EL EPSILON SE DISMINUYE 1 VEZ CADA t, PERO LA IMAGEN LA AGREGA CADA 4 t\n",
    "        if epsilon > FINAL_EPSILON and t > OBSERVATION:\n",
    "          epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / 1000000\n",
    "        \n",
    "        # si es terminal, los agrega a la deque desde la lista\n",
    "        # -------------------------------------------------------\n",
    "        if r_t != 0:\n",
    "            \n",
    "            if r_t > 0:\n",
    "            \n",
    "                for D_list in D_por_episodio:\n",
    "                    DDs_t, DDa_t, DDr_t, DDs_t1, DDterminal = D_list\n",
    "                    D.append((DDs_t, DDa_t, 4*r_t, DDs_t1, DDterminal)) # lo agrega a la memory replay con reward R_T (reward final)\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                for D_list in D_por_episodio:\n",
    "                    DDs_t, DDa_t, DDr_t, DDs_t1, DDterminal = D_list\n",
    "                    D.append((DDs_t, DDa_t, 4*r_t, DDs_t1, DDterminal)) # lo agrega a la memory replay con reward R_T (reward final)\n",
    "            \n",
    "            \n",
    "            if len(D) > REPLAY_MEMORY:\n",
    "              D.popleft()\n",
    "              \n",
    "            D_por_episodio.clear()\n",
    "        \n",
    "        #NUESTRO TIEMPO DE OBSERVACION ES CUANDO SE LLENA LA REPLAY MEMORY, UNA VEZ LLENA COMIENZA A ENTRENAR\n",
    "        if len(D) >= REPLAY_MEMORY :\n",
    "            minibatch = random.sample(D,BATCH)\n",
    "            inputs = np.zeros((BATCH,s_t.shape[1],s_t.shape[2],s_t.shape[3]))\n",
    "            targets = np.zeros((inputs.shape[0],3))\n",
    "            state_t, action_t, reward_t, state_t1, terminal_t1 = zip(*minibatch)\n",
    "            state_t = np.concatenate(state_t)\n",
    "            state_t1 = np.concatenate(state_t1)\n",
    "                      \n",
    "            targets = model.predict(state_t)\n",
    "            \n",
    "            Q_sa = model.predict(state_t1)\n",
    "            \n",
    "            \n",
    "            if terminal_t1 == True: #terminal_t1 es el terminal del minibatch\n",
    "              targets[range(BATCH), action_t] = reward_t\n",
    "              print('suma reward', reward_t)\n",
    "              \n",
    "            else:\n",
    "              targets[range(BATCH), action_t] = [x + normalizar(GAMMA*np.max(Q_sa)) for x in reward_t]\n",
    "              \n",
    "              #print([x + normalizar(GAMMA*np.max(Q_sa)) for x in reward_t])\n",
    "              \n",
    "              \n",
    "            loss += model.train_on_batch(state_t, targets)\n",
    "            entrenando = 'entrenando'\n",
    "        \n",
    "        # terminal1 es el terminal del juego (lo que acaba de jugar)\n",
    "        if terminal1==True :\n",
    "          print(\"ACABO LA PARTIDA CON REWARD \", reward_acum)\n",
    "          print(t,'Size DEQUE:', len(D),' - Action:', a_t,'- Reward_inst:',r_t1, ' - Reward acum:', reward_acum, ' - Rand_or_pred:',opcion,entrenando, 'EPSILON', epsilon)\n",
    "          terminal = False\n",
    "          reward_acum = 0\n",
    "          env.reset()\n",
    "          #time.sleep(10)\n",
    "        t += 1\n",
    "        \n",
    "        # guardamos el modelo cada 50000 iteraciones \n",
    "        if t % 50000 == 0:\n",
    "            print(\"Now we save model\")\n",
    "            model.save(\"model.h5\", overwrite=True)\n",
    "                \n",
    "            # guardamos modelo en el drive \n",
    "            #save_to_drive('model.h5')\n",
    "            \n",
    "        print(t,'Size DEQUE:', len(D),' - Action:', a_t,'- Reward_inst:',r_t1, ' - Reward acum:', reward_acum, ' - Rand_or_pred:',opcion,entrenando)\n",
    "        #mostrarImagen(s_t1)\n",
    "        \n",
    "    return epsilon\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Tarea3_Deep_Learning.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
